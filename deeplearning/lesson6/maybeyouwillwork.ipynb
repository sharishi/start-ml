{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "cvpe82np45acm2m9161r",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-06T08:27:55.724293Z",
     "iopub.status.busy": "2023-07-06T08:27:55.723949Z",
     "iopub.status.idle": "2023-07-06T08:28:01.191088Z",
     "shell.execute_reply": "2023-07-06T08:28:01.190138Z",
     "shell.execute_reply.started": "2023-07-06T08:27:55.724261Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:18:02.803605500Z",
     "start_time": "2024-04-08T10:18:00.280631400Z"
    }
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "d8pwhs5uqpc2d3adwmuqiv",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-06T08:28:01.193723Z",
     "iopub.status.busy": "2023-07-06T08:28:01.193123Z",
     "iopub.status.idle": "2023-07-06T08:28:47.479311Z",
     "shell.execute_reply": "2023-07-06T08:28:47.478218Z",
     "shell.execute_reply.started": "2023-07-06T08:28:01.193688Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:18:03.888106Z",
     "start_time": "2024-04-08T10:18:02.804608700Z"
    }
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "\n",
    "dataset = OxfordIIITPet('data', target_types='segmentation', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "9nlirgzrp7mofjzbkqazr",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-06T08:28:47.481069Z",
     "iopub.status.busy": "2023-07-06T08:28:47.480723Z",
     "iopub.status.idle": "2023-07-06T08:28:47.750382Z",
     "shell.execute_reply": "2023-07-06T08:28:47.749448Z",
     "shell.execute_reply.started": "2023-07-06T08:28:47.481035Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:18:03.933939100Z",
     "start_time": "2024-04-08T10:18:03.891104Z"
    }
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "transform = T.Compose(\n",
    "    [\n",
    "        T.Resize((256, 256)),\n",
    "        T.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize((256, 256)),\n",
    "        T.PILToTensor(),\n",
    "        T.Lambda(lambda x: (x - 1).long())\n",
    "    ]\n",
    ")\n",
    "# то нравится, то не нравится...\n",
    "# def lambda_func(x):\n",
    "#     return (x-1).long\n",
    "#     \n",
    "# transform = T.Compose(\n",
    "#     [\n",
    "#         T.Resize((256, 256)),\n",
    "#         T.ToTensor(),\n",
    "#     ]\n",
    "# )\n",
    "# target_transform = T.Compose(\n",
    "#     [\n",
    "#         T.Resize((256, 256)),\n",
    "#         T.PILToTensor(),\n",
    "#         T.Lambda(lambda_func)\n",
    "#     ]\n",
    "# )\n",
    "train_dataset = OxfordIIITPet('data', transform=transform, target_transform=target_transform, target_types='segmentation')\n",
    "valid_dataset = OxfordIIITPet('data', transform=transform, split='test', target_transform=target_transform, target_types='segmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellId": "ffs63ugzil8wz2x0tofrb7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-06T08:28:47.753117Z",
     "iopub.status.busy": "2023-07-06T08:28:47.752773Z",
     "iopub.status.idle": "2023-07-06T08:28:47.771855Z",
     "shell.execute_reply": "2023-07-06T08:28:47.770784Z",
     "shell.execute_reply.started": "2023-07-06T08:28:47.753083Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:18:04.962391800Z",
     "start_time": "2024-04-08T10:18:04.946871300Z"
    }
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "dzddyzfdiilirsiodtrnm",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-06T08:28:47.778446Z",
     "iopub.status.busy": "2023-07-06T08:28:47.773462Z",
     "iopub.status.idle": "2023-07-06T08:28:47.825842Z",
     "shell.execute_reply": "2023-07-06T08:28:47.824379Z",
     "shell.execute_reply.started": "2023-07-06T08:28:47.778402Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:18:05.999826400Z",
     "start_time": "2024-04-08T10:18:05.975814100Z"
    }
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model) -> float:\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for x, y in tqdm(train_loader, desc='Train'):\n",
    "        bs = y.size(0)\n",
    "\n",
    "        x, y = x.to(device), y.squeeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        loss = loss_fn(output.reshape(bs, 3, -1), y.reshape(bs, -1))\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        _, y_pred = output.max(dim=1)\n",
    "        total += y.size(0) * y.size(1) * y.size(2)\n",
    "        correct += (y == y_pred).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "ltg96xv7y10dphu68h0slk",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-06T08:28:47.828537Z",
     "iopub.status.busy": "2023-07-06T08:28:47.827927Z",
     "iopub.status.idle": "2023-07-06T08:28:47.895177Z",
     "shell.execute_reply": "2023-07-06T08:28:47.892884Z",
     "shell.execute_reply.started": "2023-07-06T08:28:47.828475Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:18:06.841320900Z",
     "start_time": "2024-04-08T10:18:06.825310700Z"
    }
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, loader) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for x, y in tqdm(loader, desc='Evaluation'):\n",
    "        bs = y.size(0)\n",
    "\n",
    "        x, y = x.to(device), y.squeeze(1).to(device)\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        loss = loss_fn(output.reshape(bs, 3, -1), y.reshape(bs, -1))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, y_pred = output.max(dim=1)\n",
    "        total += y.size(0) * y.size(1) * y.size(2)\n",
    "        correct += (y == y_pred).sum().item()\n",
    "\n",
    "    total_loss /= len(loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellId": "p3nfrmyozeix0mnh462i",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-06T08:28:47.897733Z",
     "iopub.status.busy": "2023-07-06T08:28:47.896957Z",
     "iopub.status.idle": "2023-07-06T08:28:47.974314Z",
     "shell.execute_reply": "2023-07-06T08:28:47.971373Z",
     "shell.execute_reply.started": "2023-07-06T08:28:47.897688Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T09:40:35.468027Z",
     "start_time": "2024-04-08T09:40:35.453028600Z"
    }
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:28:47.977010Z",
     "iopub.status.busy": "2023-07-06T08:28:47.976612Z",
     "iopub.status.idle": "2023-07-06T08:28:48.074220Z",
     "shell.execute_reply": "2023-07-06T08:28:48.072865Z",
     "shell.execute_reply.started": "2023-07-06T08:28:47.976974Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:20:28.565612900Z",
     "start_time": "2024-04-08T10:20:28.553187600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def conv_plus_conv(in_channels: int, out_channels: int):\n",
    "    \"\"\"\n",
    "    Makes UNet block\n",
    "    :param in_channels: input channels\n",
    "    :param out_channels: output channels\n",
    "    :return: UNet block\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        ),\n",
    "        nn.BatchNorm2d(num_features=out_channels),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        ),\n",
    "        nn.BatchNorm2d(num_features=out_channels),\n",
    "        nn.LeakyReLU(0.2),\n",
    "    )\n",
    "\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        base_channels = 32\n",
    "\n",
    "        self.down1 = conv_plus_conv(3, base_channels)\n",
    "        self.down2 = conv_plus_conv(base_channels, base_channels * 2)\n",
    "        self.down3 = conv_plus_conv(base_channels * 2, base_channels * 4)\n",
    "        self.down4 = conv_plus_conv(base_channels * 4, base_channels * 8)\n",
    "        self.down5 = conv_plus_conv(base_channels * 8, base_channels * 16)\n",
    "\n",
    "        self.up1 = conv_plus_conv(base_channels * 2, base_channels)\n",
    "        self.up2 = conv_plus_conv(base_channels * 4, base_channels)\n",
    "        self.up3 = conv_plus_conv(base_channels * 8, base_channels * 2)\n",
    "        self.up4 = conv_plus_conv(base_channels * 16, base_channels * 4)\n",
    "        self.up5 = conv_plus_conv(base_channels * 32, base_channels * 8)\n",
    "\n",
    "        self.bottleneck = conv_plus_conv(base_channels * 16, base_channels * 16)\n",
    "\n",
    "        self.out = nn.Conv2d(in_channels=base_channels, out_channels=3, kernel_size=1)\n",
    "\n",
    "        self.downsample = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual1 = self.down1(x)  \n",
    "        x = self.downsample(residual1) \n",
    "\n",
    "        residual2 = self.down2(x) \n",
    "        x = self.downsample(residual2) \n",
    "\n",
    "        residual3 = self.down3(x)  \n",
    "        x = self.downsample(residual3)\n",
    "\n",
    "        residual4 = self.down4(x)  \n",
    "        x = self.downsample(residual4) \n",
    "        \n",
    "        residual5 = self.down5(x)  \n",
    "        x = self.downsample(residual5) \n",
    "\n",
    " \n",
    "        x = self.bottleneck(x)  \n",
    "        \n",
    "        x = nn.functional.interpolate(x, scale_factor=2)  \n",
    "        x = torch.cat((x, residual5), dim=1)  \n",
    "        x = self.up5(x) \n",
    "       \n",
    "\n",
    "        x = nn.functional.interpolate(x, scale_factor=2)  \n",
    "        x = torch.cat((x, residual4), dim=1)  \n",
    "        x = self.up4(x) \n",
    "\n",
    "        x = nn.functional.interpolate(x, scale_factor=2) \n",
    "        x = torch.cat((x, residual3), dim=1)  \n",
    "        x = self.up3(x) \n",
    "\n",
    "        x = nn.functional.interpolate(x, scale_factor=2)  \n",
    "        x = torch.cat((x, residual2), dim=1) \n",
    "        x = self.up2(x)  \n",
    "\n",
    "        x = nn.functional.interpolate(x, scale_factor=2)  \n",
    "        x = torch.cat((x, residual1), dim=1) \n",
    "        x = self.up1(x)  \n",
    "\n",
    "        x = self.out(x)  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:28:48.078796Z",
     "iopub.status.busy": "2023-07-06T08:28:48.077963Z",
     "iopub.status.idle": "2023-07-06T08:28:52.708513Z",
     "shell.execute_reply": "2023-07-06T08:28:52.707536Z",
     "shell.execute_reply.started": "2023-07-06T08:28:48.078752Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:20:30.961939800Z",
     "start_time": "2024-04-08T10:20:29.655720400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model = UNET().to(device)\n",
    "\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "#scheduler = StepLR(optimizer, step_size=25)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:28:52.710526Z",
     "iopub.status.busy": "2023-07-06T08:28:52.710152Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:20:55.438285400Z",
     "start_time": "2024-04-08T10:20:33.564576600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/58 [00:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.32 GiB is allocated by PyTorch, and 59.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m best_valid_accuracy \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m----> 9\u001B[0m     train_loss, train_accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m     valid_loss, valid_accuracy \u001B[38;5;241m=\u001B[39m evaluate(model, valid_loader)\n\u001B[0;32m     12\u001B[0m     train_loss_history\u001B[38;5;241m.\u001B[39mappend(train_loss)\n",
      "Cell \u001B[1;32mIn[5], line 19\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model)\u001B[0m\n\u001B[0;32m     15\u001B[0m x, y \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(device), y\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     17\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 19\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(output\u001B[38;5;241m.\u001B[39mreshape(bs, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), y\u001B[38;5;241m.\u001B[39mreshape(bs, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m     23\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\PycharmProjects\\karpov_module2\\deeplearning\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\karpov_module2\\deeplearning\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[7], line 96\u001B[0m, in \u001B[0;36mUNET.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     94\u001B[0m x \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39minterpolate(x, scale_factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)  \n\u001B[0;32m     95\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((x, residual1), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m) \n\u001B[1;32m---> 96\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mup1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m  \n\u001B[0;32m     98\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout(x)  \n\u001B[0;32m    100\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\PycharmProjects\\karpov_module2\\deeplearning\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\karpov_module2\\deeplearning\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\karpov_module2\\deeplearning\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\karpov_module2\\deeplearning\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\karpov_module2\\deeplearning\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\karpov_module2\\deeplearning\\venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:175\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    168\u001B[0m     bn_training \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    170\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001B[39;00m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m--> 175\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001B[39;49;00m\n\u001B[0;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_mean\u001B[49m\n\u001B[0;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_var\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbn_training\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    185\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexponential_average_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    186\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    187\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\karpov_module2\\deeplearning\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2482\u001B[0m, in \u001B[0;36mbatch_norm\u001B[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[0;32m   2479\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training:\n\u001B[0;32m   2480\u001B[0m     _verify_batch_size(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize())\n\u001B[1;32m-> 2482\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2483\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_mean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_var\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmomentum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\n\u001B[0;32m   2484\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.32 GiB is allocated by PyTorch, and 59.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_loss_history, valid_loss_history = [], []\n",
    "train_accuracy_history, valid_accuracy_history = [], []\n",
    "\n",
    "num_epochs = 35\n",
    "\n",
    "best_valid_accuracy = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(model)\n",
    "    valid_loss, valid_accuracy = evaluate(model, valid_loader)\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    valid_loss_history.append(valid_loss)\n",
    "\n",
    "    train_accuracy_history.append(train_accuracy)\n",
    "    valid_accuracy_history.append(valid_accuracy)\n",
    "    \n",
    "    best_valid_accuracy = max(valid_accuracy, best_valid_accuracy)\n",
    "    \n",
    "    print(f'epoch = {epoch+1} with valid_accuracy = {valid_accuracy*100}')\n",
    "    print(f'epoch = {epoch+1} with best_valid_accuracy = {best_valid_accuracy*100}')\n",
    "    \n",
    "    if valid_accuracy >= 0.885:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:15:26.529976Z",
     "iopub.status.busy": "2023-07-06T08:15:26.529564Z",
     "iopub.status.idle": "2023-07-06T08:15:26.536716Z",
     "shell.execute_reply": "2023-07-06T08:15:26.535763Z",
     "shell.execute_reply.started": "2023-07-06T08:15:26.529933Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2024-04-08T09:28:18.783169200Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def predict(model: nn.Module, loader: DataLoader, device: torch.device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for x, _ in loader:\n",
    "        x = x.to(device)\n",
    "        outputs = model(x)\n",
    "        y_pred = torch.argmax(outputs, 1)\n",
    "        predictions.append(y_pred)\n",
    "    result = torch.cat(predictions)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:15:27.424322Z",
     "iopub.status.busy": "2023-07-06T08:15:27.423224Z",
     "iopub.status.idle": "2023-07-06T08:15:29.720912Z",
     "shell.execute_reply": "2023-07-06T08:15:29.719719Z",
     "shell.execute_reply.started": "2023-07-06T08:15:27.424277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "idx = np.random.randint(len(valid_dataset), size=200)\n",
    "\n",
    "test_dataset = [valid_dataset[i] for i in idx]\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "predictions = predict(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:16:12.592292Z",
     "iopub.status.busy": "2023-07-06T08:16:12.591693Z",
     "iopub.status.idle": "2023-07-06T08:16:12.600151Z",
     "shell.execute_reply": "2023-07-06T08:16:12.599016Z",
     "shell.execute_reply.started": "2023-07-06T08:16:12.592259Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T11:19:55.723435Z",
     "start_time": "2024-04-08T11:19:55.715429400Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions.unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:16:28.704652Z",
     "iopub.status.busy": "2023-07-06T08:16:28.704278Z",
     "iopub.status.idle": "2023-07-06T08:16:28.920355Z",
     "shell.execute_reply": "2023-07-06T08:16:28.919239Z",
     "shell.execute_reply.started": "2023-07-06T08:16:28.704621Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# torch.save(predictions.unsqueeze(1), 'predictions.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:22:31.926189Z",
     "iopub.status.busy": "2023-07-06T08:22:31.925766Z",
     "iopub.status.idle": "2023-07-06T08:22:31.931356Z",
     "shell.execute_reply": "2023-07-06T08:22:31.930218Z",
     "shell.execute_reply.started": "2023-07-06T08:22:31.926159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions_uint8 = predictions.unsqueeze(1).to(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:22:44.997156Z",
     "iopub.status.busy": "2023-07-06T08:22:44.996734Z",
     "iopub.status.idle": "2023-07-06T08:22:45.003202Z",
     "shell.execute_reply": "2023-07-06T08:22:45.002311Z",
     "shell.execute_reply.started": "2023-07-06T08:22:44.997128Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T11:20:04.687848400Z",
     "start_time": "2024-04-08T11:20:04.667838100Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions_uint8.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T08:23:09.316339Z",
     "iopub.status.busy": "2023-07-06T08:23:09.315960Z",
     "iopub.status.idle": "2023-07-06T08:23:09.349386Z",
     "shell.execute_reply": "2023-07-06T08:23:09.348472Z",
     "shell.execute_reply.started": "2023-07-06T08:23:09.316309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(predictions_uint8, 'predictions_uint8.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "notebookId": "130d5172-c6db-4f08-925b-fdeca6d30295",
  "notebookPath": "6/Seminar_6_Intro_to_DL.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
