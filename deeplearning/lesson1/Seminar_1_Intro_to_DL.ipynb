{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Введение. Полносвязные слои. Функции активации (ноутбук)\n",
    "\n",
    "> Начнем осваивать библиотеку `PyTorch`. Познакомимся с нейронными сетями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## План ноутбука\n",
    "\n",
    "1. Установка `PyTorch`\n",
    "1. Введение в `PyTorch`\n",
    "1. Полносвязные слои и функции активации в `PyTorch`\n",
    "1. Градиентный спуск своими руками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Установка `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Мы будем использовать библиотеку для глубинного обучения `PyTorch`, ее можно не устанавливать, будем пользоваться сайтом [kaggle.com](kaggle.com) для обучения в облаке (или с учителем?). \n",
    "\n",
    "Чтобы установить `PyTorch` локально себе на компьютер нужно ответить на два вопроса - какая у вас операционная система и есть ли у вас дискретная видеокарта (GPU) и если есть, то какого производителя. В зависимости от ваших ответов мы получаем три варианта по операционной системе - Linux, Mac и Windows; три варианта по дискретной видеокарте - нет видеокарты (доступен только центральный процессор CPU), есть видеокарта от Nvidia или есть видеокарта от AMD (это производитель именно чипа, конечный вендор может быть другой, например, ASUS, MSI, Palit). Работа с PyTorch с видеокартой от AMD это экзотика, которая выходит за рамки нашего курса, поэтому рассмотрим только варианты *нет видеокарты*/*есть видеокарта от Nvidia*.\n",
    "\n",
    "\n",
    "Выберите на [сайте](https://pytorch.org/get-started/locally/) подходящие вам варианты операционной системы/видеокарты и скопируйте команду для установки. Разберем подробно самые популярные варианты установки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка в Linux ([поддерживаемые дистрибутивы](https://pytorch.org/get-started/locally/#supported-linux-distributions))\n",
    "\n",
    "На линуксе будет работать поддержка `PyTorch` в любой конфигурации, что у вас нет видеокарты, что есть от Nvidia, что от AMD. \n",
    "\n",
    "Пререквизит для работы с видеокартой от Nvidia - нужно поставить CUDA, это инструмент от компании Nvidia, который позволяет ускорять вычисления на их же ГПУ. Чтобы поставить себе на машину все правильно воспользуйтесь этим [гайдом](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) от Nvidia.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` для тех, у кого нет видеокарты.\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116` для тех, у кого есть видеокарта (либо другой `--extra-index-url`, смотрите на сайте PyTorch, в зависимости от версии CUDA).\n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch` для тех, у кого нет видеокарты.\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge` для тех, у кого есть видеокарта (либо немного другая команда, в зависимости от версии CUDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка в Windows\n",
    "\n",
    "На винде будет работать поддержка `PyTorch` только для видеокарт от Nvidia и без видеокарт вообще. \n",
    "\n",
    "Пререквизит для работы с видеокартой от Nvidia - нужно поставить CUDA, это инструмент от компании Nvidia, который позволяет ускорять вычисления на их же ГПУ. Чтобы поставить себе на машину все правильно воспользуйтесь этим [гайдом](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html) от Nvidia.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` для тех, у кого нет видеокарты.\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116` для тех, у кого есть видеокарта (либо другой `--extra-index-url`, смотрите на сайте PyTorch, в зависимости от версии CUDA).\n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch` для тех, у кого нет видеокарты.\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge` для тех, у кого есть видеокарта (либо немного другая команда, в зависимости от версии CUDA).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка на Mac\n",
    "\n",
    "На маках есть пока что поддержка `PyTorch` только центрального процессора, чуть позже появится поддержка ускорения на чипах M1, M2, M1 Pro и так далее.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` \n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Введение в `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Тензоры\n",
    "\n",
    "Тензоры — это специализированная структура данных, по сути это массивы и матрицы. Тензоры очень похожи на массивы в numpy, так что, если у вас хорошо с numpy, то разобраться в PyTorch тензорах будет очень просто. В PyTorch мы используем тензоры для кодирования входных и выходных данных модели, а также параметров модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание тензоров\n",
    "\n",
    "Тензор можно создать напрямую из каких-то данных - нам подходят все списки с числами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [1, 2, 3, 4]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [[1, 2], [3, 4], [5, 6]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2]],\n",
       "\n",
       "        [[3],\n",
       "         [4]],\n",
       "\n",
       "        [[5],\n",
       "         [6]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [[[1], [2]], [[3], [4]], [[5], [6]]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле про \"все\" списки с числами - обман. Если у вашего списка есть какой-то уровень вложенности, то должны совпадать размерности у всех вложенных списков (подробнее про размерности поговорим позже):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 1 (got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m some_other_data \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m]]\n\u001b[0;32m----> 2\u001b[0m some_other_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msome_other_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m some_other_tensor\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 3)"
     ]
    }
   ],
   "source": [
    "some_other_data = [[1, 2], [3, 4], [5, 6, 7]]\n",
    "some_other_tensor = torch.tensor(some_other_data)\n",
    "\n",
    "some_other_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также тензоры можно создавать из numpy массивов и наоборот:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1],\n",
       "        [2]],\n",
       "\n",
       "       [[3],\n",
       "        [4]],\n",
       "\n",
       "       [[5],\n",
       "        [6]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_numpy_array = np.array(some_data)\n",
    "\n",
    "some_numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2]],\n",
       "\n",
       "        [[3],\n",
       "         [4]],\n",
       "\n",
       "        [[5],\n",
       "         [6]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tensor_from_numpy = torch.from_numpy(some_numpy_array)\n",
    "\n",
    "some_tensor_from_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом если мы создаем тензор из numpy массива с помощью `torch.from_numpy`, то они делят между собой память, где лежат их данные и, соответственно, при изменении тензора меняется numpy массив и наоборот:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones(10)\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),\n",
       " tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], dtype=torch.float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x += 1\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10)\n",
    "y = x.numpy()\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),\n",
       " array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x += 1\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем создать тензор со случайными или константными значениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3685, 0.6298, 0.6781],\n",
       "         [0.8752, 0.1232, 0.8770]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " tensor([[1.4013e-45, 0.0000e+00, 3.8344e-10],\n",
       "         [4.5678e-41, 0.0000e+00, 0.0000e+00]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 3)\n",
    "\n",
    "random_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "empty_tensor = torch.empty(shape)\n",
    "\n",
    "random_tensor, ones_tensor, zeros_tensor, empty_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь поговорим про размерности подробнее.\n",
    "\n",
    "У тензора есть какой-то размер, какая форма. Первое с чем нужно определиться, какой **размерности** тензор - количество осей у него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2560, 0.9379, 0.6417, 0.5229, 0.8069, 0.4212, 0.0097, 0.7956, 0.5459,\n",
       "        0.2644])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (10)  # одна ось (вектор)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0559, 0.2930, 0.9226],\n",
       "        [0.0079, 0.0833, 0.0602]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 3)  # две оси (матрица)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3993, 0.4952, 0.0413],\n",
       "         [0.6340, 0.6229, 0.8200]],\n",
       "\n",
       "        [[0.7292, 0.2421, 0.7074],\n",
       "         [0.0581, 0.0137, 0.0532]],\n",
       "\n",
       "        [[0.9352, 0.7882, 0.3498],\n",
       "         [0.9770, 0.3449, 0.2242]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (3, 2, 3)  # три оси (и больше - тензор)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тензор с размерностью 1 - это просто вектор, список чисел.\n",
    "\n",
    "Тензор с размерностью 2 - это просто матрица, то есть список списков чисел.\n",
    "\n",
    "Тензор с размерностью 3 и больше - это тензор, то есть список списков списков ... чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить доступ к размеру уже созданного тензора - метод `.shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1],\n",
      "         [2]],\n",
      "\n",
      "        [[3],\n",
      "         [4]],\n",
      "\n",
      "        [[5],\n",
      "         [6]]])\n",
      "torch.Size([3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "some_data = [[[1], [2]], [[3], [4]], [[5], [6]]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "print(some_tensor)\n",
    "print(some_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В лекции мы говорили про изображения, давайте сделаем тензор, который будет нам имитировать изображение - сделаем его размер `(c, h, w)`, где `h` и `w` это его высота и ширина, а `c` - число каналов в цветовом пространстве (в черно-белом 1, в RGB 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[8.3746e-01, 6.0244e-01, 2.8988e-02, 2.3000e-01, 1.6531e-01,\n",
       "          8.2906e-02, 8.5505e-01, 4.3812e-01, 4.2164e-01, 8.8205e-01,\n",
       "          3.5867e-01, 4.0874e-01, 9.7402e-01, 6.2631e-01, 7.3865e-01,\n",
       "          7.5694e-01],\n",
       "         [4.0769e-01, 5.6469e-01, 3.7973e-02, 7.6929e-01, 4.8907e-01,\n",
       "          6.1830e-01, 5.2544e-01, 8.9069e-01, 4.6612e-01, 2.5656e-01,\n",
       "          6.2650e-01, 4.7899e-01, 9.7939e-01, 4.0836e-01, 1.0566e-01,\n",
       "          4.2159e-02],\n",
       "         [8.1320e-01, 8.4103e-01, 5.5651e-01, 2.5494e-01, 2.8728e-01,\n",
       "          7.5755e-02, 1.4035e-02, 8.6859e-01, 4.6440e-01, 8.4055e-01,\n",
       "          5.4293e-01, 1.4610e-01, 7.2427e-01, 5.9552e-01, 4.0405e-01,\n",
       "          2.5422e-01],\n",
       "         [2.3775e-02, 3.8704e-01, 9.4980e-01, 2.6257e-01, 1.4502e-01,\n",
       "          3.1329e-01, 3.2228e-01, 7.9406e-01, 5.9280e-01, 9.5976e-01,\n",
       "          4.4229e-01, 3.7042e-01, 8.1824e-01, 7.5102e-01, 3.4006e-01,\n",
       "          7.4669e-01],\n",
       "         [8.4726e-01, 8.7349e-01, 6.7073e-01, 1.0276e-01, 7.5686e-01,\n",
       "          6.4135e-01, 8.9659e-02, 5.0494e-01, 1.8133e-01, 8.7930e-01,\n",
       "          6.9349e-01, 4.6930e-01, 8.2198e-01, 4.6494e-01, 7.7575e-01,\n",
       "          1.2574e-01],\n",
       "         [1.4657e-01, 9.1129e-01, 9.5438e-01, 7.4646e-01, 7.2543e-01,\n",
       "          1.4110e-01, 2.4408e-01, 1.1553e-01, 2.3461e-01, 9.5611e-01,\n",
       "          4.5407e-01, 8.7488e-01, 3.8401e-01, 8.5945e-01, 8.2304e-01,\n",
       "          8.1743e-01],\n",
       "         [5.5141e-01, 8.0082e-01, 9.0737e-01, 9.5128e-01, 4.7521e-01,\n",
       "          8.8693e-01, 4.0505e-01, 6.3238e-02, 1.4842e-02, 2.1788e-01,\n",
       "          9.5565e-01, 1.1196e-01, 1.4742e-01, 6.8721e-01, 8.1428e-02,\n",
       "          1.5773e-01],\n",
       "         [3.7578e-01, 9.2181e-01, 8.0982e-01, 1.7486e-01, 9.9905e-01,\n",
       "          1.3831e-01, 5.3646e-01, 2.5770e-01, 8.6497e-01, 9.2183e-01,\n",
       "          8.7924e-01, 2.4205e-02, 4.7809e-01, 3.6923e-01, 9.2447e-01,\n",
       "          7.1047e-01],\n",
       "         [8.9607e-01, 2.8359e-01, 8.9484e-01, 9.4159e-01, 6.8408e-01,\n",
       "          5.8600e-01, 7.3046e-01, 5.9061e-01, 5.6918e-01, 1.4695e-02,\n",
       "          8.3368e-01, 1.8713e-01, 1.4146e-01, 5.2154e-01, 7.2982e-02,\n",
       "          1.9186e-01]],\n",
       "\n",
       "        [[5.7681e-01, 9.5007e-01, 6.6546e-01, 9.9677e-02, 4.8283e-01,\n",
       "          4.5187e-01, 8.4474e-01, 4.3006e-02, 8.0446e-02, 1.1192e-01,\n",
       "          9.9032e-01, 4.2411e-01, 5.8495e-01, 8.3795e-01, 9.1958e-01,\n",
       "          4.3732e-01],\n",
       "         [3.9968e-02, 4.2757e-01, 9.5835e-01, 3.8614e-01, 4.4868e-01,\n",
       "          5.1837e-01, 3.8002e-01, 8.4496e-01, 3.8838e-01, 5.6290e-01,\n",
       "          3.4025e-01, 8.6620e-01, 3.5196e-01, 7.0632e-01, 5.3633e-01,\n",
       "          5.1020e-01],\n",
       "         [7.8656e-01, 1.6652e-01, 2.1104e-01, 8.0741e-01, 1.9269e-01,\n",
       "          1.2140e-01, 2.4964e-01, 4.8721e-01, 5.6768e-01, 5.7991e-01,\n",
       "          4.8268e-01, 7.4781e-01, 4.0763e-01, 8.1776e-01, 6.2582e-01,\n",
       "          6.6310e-01],\n",
       "         [4.7385e-01, 6.4555e-01, 4.2377e-01, 8.9908e-01, 4.1122e-01,\n",
       "          7.2543e-01, 1.5725e-01, 4.0210e-01, 4.4266e-02, 2.9872e-02,\n",
       "          1.3195e-01, 9.5252e-01, 2.7423e-01, 4.9663e-02, 7.7890e-01,\n",
       "          8.6743e-01],\n",
       "         [6.4608e-01, 3.7701e-01, 3.4263e-01, 9.2517e-01, 2.2627e-01,\n",
       "          8.3936e-01, 4.8726e-01, 2.0056e-01, 5.5450e-01, 8.9145e-01,\n",
       "          6.2672e-01, 8.7201e-01, 1.7449e-01, 4.9221e-02, 3.7150e-01,\n",
       "          3.3168e-01],\n",
       "         [7.7054e-01, 7.4945e-01, 3.0797e-01, 3.8694e-01, 1.4812e-01,\n",
       "          3.0579e-01, 3.0854e-01, 2.7258e-01, 6.5143e-01, 1.9783e-01,\n",
       "          2.6792e-01, 7.4401e-01, 1.7715e-01, 4.6040e-01, 9.5253e-01,\n",
       "          6.3855e-01],\n",
       "         [7.3459e-01, 3.4735e-01, 3.5259e-01, 3.6741e-01, 9.7747e-01,\n",
       "          2.7661e-01, 9.4579e-01, 6.9275e-01, 7.0150e-01, 4.4507e-01,\n",
       "          2.2678e-01, 5.8635e-01, 9.9826e-01, 9.4778e-01, 7.7233e-01,\n",
       "          4.0640e-01],\n",
       "         [6.7257e-01, 1.4197e-01, 9.9911e-01, 5.0966e-01, 6.7281e-01,\n",
       "          7.3171e-02, 4.4383e-01, 9.6166e-02, 8.6693e-01, 5.3268e-01,\n",
       "          8.3709e-01, 7.2375e-01, 2.5067e-01, 8.9841e-01, 6.2708e-01,\n",
       "          2.4055e-01],\n",
       "         [6.9629e-01, 7.8302e-01, 4.3205e-01, 3.8893e-01, 4.7610e-01,\n",
       "          2.1802e-01, 7.5267e-01, 9.0640e-01, 1.7037e-01, 9.5654e-01,\n",
       "          5.0072e-01, 6.8959e-01, 2.1020e-01, 4.8230e-01, 9.9135e-01,\n",
       "          6.4229e-01]],\n",
       "\n",
       "        [[4.9230e-01, 8.2935e-01, 6.4997e-01, 5.2138e-01, 8.4348e-01,\n",
       "          1.5301e-01, 2.1547e-02, 4.1019e-01, 5.3859e-01, 2.4350e-01,\n",
       "          8.7172e-01, 3.5163e-01, 4.1227e-02, 7.3519e-01, 7.9933e-01,\n",
       "          9.5914e-02],\n",
       "         [1.8910e-01, 7.3901e-01, 6.5097e-01, 1.3846e-01, 4.6979e-01,\n",
       "          1.0481e-01, 7.9701e-01, 2.2462e-01, 9.0391e-01, 5.5441e-01,\n",
       "          2.0663e-01, 8.4544e-01, 4.8927e-01, 8.8182e-01, 3.9446e-01,\n",
       "          8.8688e-01],\n",
       "         [3.7822e-01, 1.3436e-01, 7.1106e-01, 3.9604e-01, 1.0490e-01,\n",
       "          1.9299e-01, 7.5989e-01, 4.1995e-01, 4.2446e-01, 1.1474e-01,\n",
       "          6.8912e-01, 9.0546e-01, 4.8278e-02, 2.7937e-01, 1.4057e-01,\n",
       "          8.6655e-01],\n",
       "         [4.7901e-01, 5.1961e-01, 1.2433e-01, 5.8937e-01, 7.2440e-01,\n",
       "          7.6433e-01, 9.2005e-01, 3.6060e-01, 3.6495e-01, 1.7252e-01,\n",
       "          2.5696e-01, 6.2162e-01, 8.2886e-01, 5.2128e-02, 6.9091e-01,\n",
       "          5.2150e-01],\n",
       "         [2.8933e-01, 7.2687e-01, 2.9673e-01, 9.7509e-01, 7.0472e-01,\n",
       "          7.7379e-01, 5.0101e-01, 9.3187e-01, 5.8175e-01, 1.8146e-01,\n",
       "          7.7290e-01, 7.3337e-01, 5.1827e-01, 4.1692e-01, 9.8466e-01,\n",
       "          3.2723e-01],\n",
       "         [9.8456e-01, 9.0505e-01, 4.7447e-01, 1.9148e-01, 7.2158e-01,\n",
       "          2.9333e-01, 2.1507e-01, 2.7655e-01, 2.3457e-01, 2.3863e-01,\n",
       "          9.3877e-04, 5.1838e-01, 9.5723e-01, 9.9299e-01, 7.6855e-01,\n",
       "          5.6271e-01],\n",
       "         [8.0616e-01, 6.8527e-01, 6.1193e-01, 2.2153e-02, 2.4713e-02,\n",
       "          1.6628e-01, 7.5750e-01, 1.3652e-01, 6.5426e-01, 9.0197e-01,\n",
       "          7.2524e-01, 3.2451e-01, 8.1316e-01, 9.4379e-01, 1.8574e-01,\n",
       "          9.5239e-01],\n",
       "         [7.4207e-01, 2.3970e-01, 4.1352e-01, 1.5769e-01, 9.7702e-01,\n",
       "          8.5686e-01, 7.5763e-01, 4.5879e-01, 6.4362e-01, 9.0336e-02,\n",
       "          6.7541e-01, 7.4023e-01, 3.3813e-02, 6.3045e-01, 9.8242e-02,\n",
       "          5.6516e-01],\n",
       "         [2.6950e-01, 1.3636e-01, 6.2335e-01, 3.2662e-01, 7.9383e-01,\n",
       "          4.2801e-01, 2.6380e-01, 9.4979e-02, 3.5555e-01, 4.1756e-01,\n",
       "          2.2727e-01, 9.8579e-01, 3.2289e-01, 5.2473e-01, 5.9164e-01,\n",
       "          2.4516e-01]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 9\n",
    "w = 16\n",
    "c = 3\n",
    "\n",
    "shape = (c, h, w)\n",
    "\n",
    "image_tensor = torch.rand(shape)\n",
    "\n",
    "image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 9, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем попробовать поменять размер тензора, например, [вытянуть его в вектор](https://pytorch.org/docs/stable/generated/torch.ravel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.3746e-01, 6.0244e-01, 2.8988e-02, 2.3000e-01, 1.6531e-01, 8.2906e-02,\n",
       "        8.5505e-01, 4.3812e-01, 4.2164e-01, 8.8205e-01, 3.5867e-01, 4.0874e-01,\n",
       "        9.7402e-01, 6.2631e-01, 7.3865e-01, 7.5694e-01, 4.0769e-01, 5.6469e-01,\n",
       "        3.7973e-02, 7.6929e-01, 4.8907e-01, 6.1830e-01, 5.2544e-01, 8.9069e-01,\n",
       "        4.6612e-01, 2.5656e-01, 6.2650e-01, 4.7899e-01, 9.7939e-01, 4.0836e-01,\n",
       "        1.0566e-01, 4.2159e-02, 8.1320e-01, 8.4103e-01, 5.5651e-01, 2.5494e-01,\n",
       "        2.8728e-01, 7.5755e-02, 1.4035e-02, 8.6859e-01, 4.6440e-01, 8.4055e-01,\n",
       "        5.4293e-01, 1.4610e-01, 7.2427e-01, 5.9552e-01, 4.0405e-01, 2.5422e-01,\n",
       "        2.3775e-02, 3.8704e-01, 9.4980e-01, 2.6257e-01, 1.4502e-01, 3.1329e-01,\n",
       "        3.2228e-01, 7.9406e-01, 5.9280e-01, 9.5976e-01, 4.4229e-01, 3.7042e-01,\n",
       "        8.1824e-01, 7.5102e-01, 3.4006e-01, 7.4669e-01, 8.4726e-01, 8.7349e-01,\n",
       "        6.7073e-01, 1.0276e-01, 7.5686e-01, 6.4135e-01, 8.9659e-02, 5.0494e-01,\n",
       "        1.8133e-01, 8.7930e-01, 6.9349e-01, 4.6930e-01, 8.2198e-01, 4.6494e-01,\n",
       "        7.7575e-01, 1.2574e-01, 1.4657e-01, 9.1129e-01, 9.5438e-01, 7.4646e-01,\n",
       "        7.2543e-01, 1.4110e-01, 2.4408e-01, 1.1553e-01, 2.3461e-01, 9.5611e-01,\n",
       "        4.5407e-01, 8.7488e-01, 3.8401e-01, 8.5945e-01, 8.2304e-01, 8.1743e-01,\n",
       "        5.5141e-01, 8.0082e-01, 9.0737e-01, 9.5128e-01, 4.7521e-01, 8.8693e-01,\n",
       "        4.0505e-01, 6.3238e-02, 1.4842e-02, 2.1788e-01, 9.5565e-01, 1.1196e-01,\n",
       "        1.4742e-01, 6.8721e-01, 8.1428e-02, 1.5773e-01, 3.7578e-01, 9.2181e-01,\n",
       "        8.0982e-01, 1.7486e-01, 9.9905e-01, 1.3831e-01, 5.3646e-01, 2.5770e-01,\n",
       "        8.6497e-01, 9.2183e-01, 8.7924e-01, 2.4205e-02, 4.7809e-01, 3.6923e-01,\n",
       "        9.2447e-01, 7.1047e-01, 8.9607e-01, 2.8359e-01, 8.9484e-01, 9.4159e-01,\n",
       "        6.8408e-01, 5.8600e-01, 7.3046e-01, 5.9061e-01, 5.6918e-01, 1.4695e-02,\n",
       "        8.3368e-01, 1.8713e-01, 1.4146e-01, 5.2154e-01, 7.2982e-02, 1.9186e-01,\n",
       "        5.7681e-01, 9.5007e-01, 6.6546e-01, 9.9677e-02, 4.8283e-01, 4.5187e-01,\n",
       "        8.4474e-01, 4.3006e-02, 8.0446e-02, 1.1192e-01, 9.9032e-01, 4.2411e-01,\n",
       "        5.8495e-01, 8.3795e-01, 9.1958e-01, 4.3732e-01, 3.9968e-02, 4.2757e-01,\n",
       "        9.5835e-01, 3.8614e-01, 4.4868e-01, 5.1837e-01, 3.8002e-01, 8.4496e-01,\n",
       "        3.8838e-01, 5.6290e-01, 3.4025e-01, 8.6620e-01, 3.5196e-01, 7.0632e-01,\n",
       "        5.3633e-01, 5.1020e-01, 7.8656e-01, 1.6652e-01, 2.1104e-01, 8.0741e-01,\n",
       "        1.9269e-01, 1.2140e-01, 2.4964e-01, 4.8721e-01, 5.6768e-01, 5.7991e-01,\n",
       "        4.8268e-01, 7.4781e-01, 4.0763e-01, 8.1776e-01, 6.2582e-01, 6.6310e-01,\n",
       "        4.7385e-01, 6.4555e-01, 4.2377e-01, 8.9908e-01, 4.1122e-01, 7.2543e-01,\n",
       "        1.5725e-01, 4.0210e-01, 4.4266e-02, 2.9872e-02, 1.3195e-01, 9.5252e-01,\n",
       "        2.7423e-01, 4.9663e-02, 7.7890e-01, 8.6743e-01, 6.4608e-01, 3.7701e-01,\n",
       "        3.4263e-01, 9.2517e-01, 2.2627e-01, 8.3936e-01, 4.8726e-01, 2.0056e-01,\n",
       "        5.5450e-01, 8.9145e-01, 6.2672e-01, 8.7201e-01, 1.7449e-01, 4.9221e-02,\n",
       "        3.7150e-01, 3.3168e-01, 7.7054e-01, 7.4945e-01, 3.0797e-01, 3.8694e-01,\n",
       "        1.4812e-01, 3.0579e-01, 3.0854e-01, 2.7258e-01, 6.5143e-01, 1.9783e-01,\n",
       "        2.6792e-01, 7.4401e-01, 1.7715e-01, 4.6040e-01, 9.5253e-01, 6.3855e-01,\n",
       "        7.3459e-01, 3.4735e-01, 3.5259e-01, 3.6741e-01, 9.7747e-01, 2.7661e-01,\n",
       "        9.4579e-01, 6.9275e-01, 7.0150e-01, 4.4507e-01, 2.2678e-01, 5.8635e-01,\n",
       "        9.9826e-01, 9.4778e-01, 7.7233e-01, 4.0640e-01, 6.7257e-01, 1.4197e-01,\n",
       "        9.9911e-01, 5.0966e-01, 6.7281e-01, 7.3171e-02, 4.4383e-01, 9.6166e-02,\n",
       "        8.6693e-01, 5.3268e-01, 8.3709e-01, 7.2375e-01, 2.5067e-01, 8.9841e-01,\n",
       "        6.2708e-01, 2.4055e-01, 6.9629e-01, 7.8302e-01, 4.3205e-01, 3.8893e-01,\n",
       "        4.7610e-01, 2.1802e-01, 7.5267e-01, 9.0640e-01, 1.7037e-01, 9.5654e-01,\n",
       "        5.0072e-01, 6.8959e-01, 2.1020e-01, 4.8230e-01, 9.9135e-01, 6.4229e-01,\n",
       "        4.9230e-01, 8.2935e-01, 6.4997e-01, 5.2138e-01, 8.4348e-01, 1.5301e-01,\n",
       "        2.1547e-02, 4.1019e-01, 5.3859e-01, 2.4350e-01, 8.7172e-01, 3.5163e-01,\n",
       "        4.1227e-02, 7.3519e-01, 7.9933e-01, 9.5914e-02, 1.8910e-01, 7.3901e-01,\n",
       "        6.5097e-01, 1.3846e-01, 4.6979e-01, 1.0481e-01, 7.9701e-01, 2.2462e-01,\n",
       "        9.0391e-01, 5.5441e-01, 2.0663e-01, 8.4544e-01, 4.8927e-01, 8.8182e-01,\n",
       "        3.9446e-01, 8.8688e-01, 3.7822e-01, 1.3436e-01, 7.1106e-01, 3.9604e-01,\n",
       "        1.0490e-01, 1.9299e-01, 7.5989e-01, 4.1995e-01, 4.2446e-01, 1.1474e-01,\n",
       "        6.8912e-01, 9.0546e-01, 4.8278e-02, 2.7937e-01, 1.4057e-01, 8.6655e-01,\n",
       "        4.7901e-01, 5.1961e-01, 1.2433e-01, 5.8937e-01, 7.2440e-01, 7.6433e-01,\n",
       "        9.2005e-01, 3.6060e-01, 3.6495e-01, 1.7252e-01, 2.5696e-01, 6.2162e-01,\n",
       "        8.2886e-01, 5.2128e-02, 6.9091e-01, 5.2150e-01, 2.8933e-01, 7.2687e-01,\n",
       "        2.9673e-01, 9.7509e-01, 7.0472e-01, 7.7379e-01, 5.0101e-01, 9.3187e-01,\n",
       "        5.8175e-01, 1.8146e-01, 7.7290e-01, 7.3337e-01, 5.1827e-01, 4.1692e-01,\n",
       "        9.8466e-01, 3.2723e-01, 9.8456e-01, 9.0505e-01, 4.7447e-01, 1.9148e-01,\n",
       "        7.2158e-01, 2.9333e-01, 2.1507e-01, 2.7655e-01, 2.3457e-01, 2.3863e-01,\n",
       "        9.3877e-04, 5.1838e-01, 9.5723e-01, 9.9299e-01, 7.6855e-01, 5.6271e-01,\n",
       "        8.0616e-01, 6.8527e-01, 6.1193e-01, 2.2153e-02, 2.4713e-02, 1.6628e-01,\n",
       "        7.5750e-01, 1.3652e-01, 6.5426e-01, 9.0197e-01, 7.2524e-01, 3.2451e-01,\n",
       "        8.1316e-01, 9.4379e-01, 1.8574e-01, 9.5239e-01, 7.4207e-01, 2.3970e-01,\n",
       "        4.1352e-01, 1.5769e-01, 9.7702e-01, 8.5686e-01, 7.5763e-01, 4.5879e-01,\n",
       "        6.4362e-01, 9.0336e-02, 6.7541e-01, 7.4023e-01, 3.3813e-02, 6.3045e-01,\n",
       "        9.8242e-02, 5.6516e-01, 2.6950e-01, 1.3636e-01, 6.2335e-01, 3.2662e-01,\n",
       "        7.9383e-01, 4.2801e-01, 2.6380e-01, 9.4979e-02, 3.5555e-01, 4.1756e-01,\n",
       "        2.2727e-01, 9.8579e-01, 3.2289e-01, 5.2473e-01, 5.9164e-01, 2.4516e-01])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([432])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h * w * c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество элементов в тензоре с помощью [специальной функции](https://pytorch.org/docs/stable/generated/torch.numel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0260, 0.5321, 0.8319],\n",
       "         [0.8304, 0.5789, 0.3981]],\n",
       "\n",
       "        [[0.5885, 0.2135, 0.3219],\n",
       "         [0.3482, 0.2303, 0.0958]],\n",
       "\n",
       "        [[0.6992, 0.2370, 0.6614],\n",
       "         [0.0874, 0.5682, 0.1406]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 2\n",
    "w = 3\n",
    "c = 3\n",
    "\n",
    "shape = (c, h, w)\n",
    "\n",
    "image_tensor = torch.rand(shape)\n",
    "\n",
    "image_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем поменять размер с помощью функции [reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0260, 0.5321, 0.8319, 0.8304, 0.5789, 0.3981],\n",
       "        [0.5885, 0.2135, 0.3219, 0.3482, 0.2303, 0.0958],\n",
       "        [0.6992, 0.2370, 0.6614, 0.0874, 0.5682, 0.1406]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.reshape(c, h * w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем собрать из нескольких тензоров один большой:\n",
    "\n",
    "[torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0242,  0.5519, -0.2173],\n",
       "        [ 0.6458,  0.1329, -0.0684]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0242,  0.5519, -0.2173],\n",
       "        [ 0.6458,  0.1329, -0.0684],\n",
       "        [ 1.0242,  0.5519, -0.2173],\n",
       "        [ 0.6458,  0.1329, -0.0684],\n",
       "        [ 1.0242,  0.5519, -0.2173],\n",
       "        [ 0.6458,  0.1329, -0.0684]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, x, x), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0242,  0.5519, -0.2173,  1.0242,  0.5519, -0.2173,  1.0242,  0.5519,\n",
       "         -0.2173],\n",
       "        [ 0.6458,  0.1329, -0.0684,  0.6458,  0.1329, -0.0684,  0.6458,  0.1329,\n",
       "         -0.0684]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, x, x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0177,  0.4162, -0.4171],\n",
      "        [ 0.7492, -0.7409,  1.5397],\n",
      "        [-1.4340,  0.3700, -0.0370]])\n",
      "tensor([[-1.4738, -0.8240, -1.8620],\n",
      "        [ 0.4807, -0.3187,  0.8467],\n",
      "        [ 0.9291, -0.2473, -0.4379],\n",
      "        [-0.6789,  0.4848, -0.5614],\n",
      "        [-0.9092,  0.7024, -0.3369]])\n",
      "tensor([[ 1.1763,  0.2095, -0.8906]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0177,  0.4162, -0.4171],\n",
       "        [ 0.7492, -0.7409,  1.5397],\n",
       "        [-1.4340,  0.3700, -0.0370],\n",
       "        [-1.4738, -0.8240, -1.8620],\n",
       "        [ 0.4807, -0.3187,  0.8467],\n",
       "        [ 0.9291, -0.2473, -0.4379],\n",
       "        [-0.6789,  0.4848, -0.5614],\n",
       "        [-0.9092,  0.7024, -0.3369],\n",
       "        [ 1.1763,  0.2095, -0.8906]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(5, 3)\n",
    "z = torch.randn(1, 3)\n",
    "\n",
    "for tensor in [x, y, z]:\n",
    "    print(tensor)\n",
    "\n",
    "torch.cat((x, y, z), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7081, -0.5018, -0.8685],\n",
      "        [ 0.7399, -0.4690,  2.0341]])\n",
      "tensor([[ 0.1744,  1.2630,  0.2382, -0.8061, -0.4706],\n",
      "        [-0.1806, -1.1920, -0.1311, -1.4321,  1.1250]])\n",
      "tensor([[-1.3549],\n",
      "        [-1.2424]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7081, -0.5018, -0.8685,  0.1744,  1.2630,  0.2382, -0.8061, -0.4706,\n",
       "         -1.3549],\n",
       "        [ 0.7399, -0.4690,  2.0341, -0.1806, -1.1920, -0.1311, -1.4321,  1.1250,\n",
       "         -1.2424]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "y = torch.randn(2, 5)\n",
    "z = torch.randn(2, 1)\n",
    "\n",
    "for tensor in [x, y, z]:\n",
    "    print(tensor)\n",
    "\n",
    "torch.cat((x, y, z), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавим дополнительную ось:\n",
    "\n",
    "[torch.unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7055, 0.5216, 0.3874],\n",
      "        [0.2206, 0.0283, 0.8868]])\n",
      "\n",
      "tensor([[[0.7055, 0.5216, 0.3874],\n",
      "         [0.2206, 0.0283, 0.8868]]]) torch.Size([1, 2, 3])\n",
      "\n",
      "tensor([[[0.7055, 0.5216, 0.3874]],\n",
      "\n",
      "        [[0.2206, 0.0283, 0.8868]]]) torch.Size([2, 1, 3])\n",
      "\n",
      "tensor([[[0.7055],\n",
      "         [0.5216],\n",
      "         [0.3874]],\n",
      "\n",
      "        [[0.2206],\n",
      "         [0.0283],\n",
      "         [0.8868]]]) torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(x.unsqueeze(0), x.unsqueeze(0).shape)\n",
    "print()\n",
    "print(x.unsqueeze(1), x.unsqueeze(1).shape)\n",
    "print()\n",
    "print(x.unsqueeze(2), x.unsqueeze(2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уберем лишние оси (где размер единичка):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.9183, 0.6002, 0.5868]],\n",
      "\n",
      "         [[0.1198, 0.9294, 0.1835]]]])\n",
      "\n",
      "tensor([[0.9183, 0.6002, 0.5868],\n",
      "        [0.1198, 0.9294, 0.1835]]) torch.Size([2, 3])\n",
      "\n",
      "tensor([[[0.9183, 0.6002, 0.5868]],\n",
      "\n",
      "        [[0.1198, 0.9294, 0.1835]]]) torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 2, 1, 3)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(x.squeeze(), x.squeeze().shape)\n",
    "print()\n",
    "print(x.squeeze(0), x.squeeze(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь поговорим про типы данных в тензорах. По умолчанию в тензорах лежат числа в torch.float32 для вещественных и torch.int64 для целочисленных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.2000, 3.7000, 4.9000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9])\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.1992, 3.6992, 4.8984], dtype=torch.float16)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9], dtype=torch.float16)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.2000, 3.7000, 4.9000], dtype=torch.float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9], dtype=torch.float64)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49])\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], dtype=torch.int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], dtype=torch.int32)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], dtype=torch.int16)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], dtype=torch.int16)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размещение тензора на GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3090 Ti\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 11 21:46:39 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:09:00.0  On |                    0 |\r\n",
      "| 30%   32C    P5    31W / 450W |    715MiB / 23028MiB |     37%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1952      G   /usr/lib/xorg/Xorg                509MiB |\r\n",
      "|    0   N/A  N/A      2177      G   /usr/bin/gnome-shell              156MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], device=device)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15, 22, 37, 49])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49])\n",
    "\n",
    "print(tensor)\n",
    "\n",
    "tensor = tensor.to(device)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.to(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor.cpu()\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7184, 0.5498, 0.5604],\n",
       "        [0.4750, 1.2123, 0.9842]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "a = a.to(device)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7184, 0.5498, 0.5604],\n",
       "        [0.4750, 1.2123, 0.9842]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b.to(device)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Операции с тензорами\n",
    "\n",
    "Большая часть операций с тензорами хорошо описана в их [документации](https://pytorch.org/docs/stable/torch.html), разберем основные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0542, 0.3823, 0.5019],\n",
       "         [0.7535, 0.0122, 0.9364]]),\n",
       " tensor([[0.9840, 0.0882, 0.7651],\n",
       "         [0.7870, 0.4985, 0.0727]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0383, 0.4705, 1.2670],\n",
      "        [1.5404, 0.5107, 1.0090]])\n",
      "\n",
      "tensor([[1.0383, 0.4705, 1.2670],\n",
      "        [1.5404, 0.5107, 1.0090]])\n",
      "\n",
      "tensor([[1.0383, 0.4705, 1.2670],\n",
      "        [1.5404, 0.5107, 1.0090]])\n"
     ]
    }
   ],
   "source": [
    "# поэлементные\n",
    "\n",
    "print(a + b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.add(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.add(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9298,  0.2940, -0.2632],\n",
      "        [-0.0335, -0.4864,  0.8637]])\n",
      "\n",
      "tensor([[-0.9298,  0.2940, -0.2632],\n",
      "        [-0.0335, -0.4864,  0.8637]])\n",
      "\n",
      "tensor([[-0.9298,  0.2940, -0.2632],\n",
      "        [-0.0335, -0.4864,  0.8637]])\n"
     ]
    }
   ],
   "source": [
    "print(a - b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.sub(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.sub(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0534, 0.0337, 0.3840],\n",
      "        [0.5929, 0.0061, 0.0680]])\n",
      "\n",
      "tensor([[0.0534, 0.0337, 0.3840],\n",
      "        [0.5929, 0.0061, 0.0680]])\n",
      "\n",
      "tensor([[0.0534, 0.0337, 0.3840],\n",
      "        [0.5929, 0.0061, 0.0680]])\n"
     ]
    }
   ],
   "source": [
    "print(a * b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.mul(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.mul(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0551,  4.3328,  0.6560],\n",
      "        [ 0.9574,  0.0244, 12.8874]])\n",
      "\n",
      "tensor([[ 0.0551,  4.3328,  0.6560],\n",
      "        [ 0.9574,  0.0244, 12.8874]])\n",
      "\n",
      "tensor([[ 0.0551,  4.3328,  0.6560],\n",
      "        [ 0.9574,  0.0244, 12.8874]])\n"
     ]
    }
   ],
   "source": [
    "print(a / b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.div(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.div(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9952, 0.3948, 0.6624],\n",
       "         [0.8935, 0.0395, 0.9213]]),\n",
       " tensor([[0.2513, 0.2235, 0.2446, 0.0461],\n",
       "         [0.7737, 0.9193, 0.6260, 0.1214],\n",
       "         [0.0117, 0.7833, 0.6692, 0.1084]]),\n",
       " tensor([[0.0264, 0.9775, 0.4449, 0.3109, 0.7528],\n",
       "         [0.3279, 0.7904, 0.5625, 0.1013, 0.3399],\n",
       "         [0.4000, 0.6632, 0.5702, 0.9594, 0.9428],\n",
       "         [0.8072, 0.0614, 0.3238, 0.0036, 0.6157],\n",
       "         [0.4995, 0.8749, 0.2871, 0.5104, 0.5305]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(3, 4)\n",
    "c = torch.rand(5, 5)\n",
    "\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5633, 1.1042, 0.9339, 0.1656],\n",
      "        [0.2658, 0.9576, 0.8598, 0.1459]]) torch.Size([2, 4])\n",
      "\n",
      "tensor([[0.5633, 1.1042, 0.9339, 0.1656],\n",
      "        [0.2658, 0.9576, 0.8598, 0.1459]]) torch.Size([2, 4])\n",
      "\n",
      "tensor(1.9212)\n",
      "\n",
      "tensor([[1.0268, 2.6579, 1.5603, 1.3647, 2.1229],\n",
      "        [1.3880, 2.2043, 1.7551, 1.1066, 1.4048],\n",
      "        [1.4919, 1.9409, 1.7685, 2.6100, 2.5671],\n",
      "        [2.2417, 1.0633, 1.3823, 1.0036, 1.8509],\n",
      "        [1.6479, 2.3987, 1.3325, 1.6659, 1.6998]])\n"
     ]
    }
   ],
   "source": [
    "# матричные операции\n",
    "\n",
    "print(a @ b, (a @ b).shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.matmul(a, b), torch.matmul(a, b).shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(c.trace())\n",
    "\n",
    "print()\n",
    "\n",
    "print(c.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### [Автоматическое дифференцирование](https://pytorch.org/docs/stable/notes/autograd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7076, 0.3914, 0.9870, 0.3090, 0.6262])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3531, 0.4118, 0.7373, 0.5783, 0.4483],\n",
       "        [0.3005, 0.9868, 0.6453, 0.5094, 0.3786],\n",
       "        [0.6972, 0.2405, 0.4653, 0.5081, 0.5772]], requires_grad=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7320e-12,  3.0774e-41, -1.7366e-12])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_z = torch.empty(3)\n",
    "\n",
    "first_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5983, 1.6304, 1.5652], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    first_z[i] = torch.sum(w[i] * x)\n",
    "\n",
    "first_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5983, 1.6304, 1.5652], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.matmul(x, w.t())\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1148, 0.7261, 0.6116], requires_grad=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.rand(3, requires_grad=True)\n",
    "\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3244, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.sum(z * v)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3244199752807617"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.mean((y - 2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1052, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad=None\n",
      "\n",
      "w.grad=None\n",
      "\n",
      "z.grad=None\n",
      "\n",
      "v.grad=None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuke/.local/lib/python3.10/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:477.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "print(f'{x.grad=}\\n')\n",
    "print(f'{w.grad=}\\n')\n",
    "print(f'{z.grad=}\\n')\n",
    "print(f'{v.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad=None\n",
      "\n",
      "w.grad=tensor([[0.0527, 0.0291, 0.0735, 0.0230, 0.0466],\n",
      "        [0.3334, 0.1844, 0.4650, 0.1456, 0.2950],\n",
      "        [0.2808, 0.1553, 0.3917, 0.1226, 0.2485]])\n",
      "\n",
      "z.grad=None\n",
      "\n",
      "v.grad=tensor([1.0370, 1.0579, 1.0155])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{x.grad=}\\n')\n",
    "print(f'{w.grad=}\\n')\n",
    "print(f'{z.grad=}\\n')\n",
    "print(f'{v.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5492], requires_grad=True), tensor([0.4124], requires_grad=True))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1368], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([2.])\n",
      "\n",
      "b.grad=tensor([-2.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # 1\n",
    "print(f'{b.grad=}\\n')  # -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0187], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (a - b) ** 2\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([0.])\n",
      "\n",
      "b.grad=tensor([0.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([0.2736])\n",
      "\n",
      "b.grad=tensor([-0.2736])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # 2 * (a - b)\n",
    "print(f'{b.grad=}\\n')  # -2 * (a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2736], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * (a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3279, 0.5942, 0.6495, 0.8076, 0.6961],\n",
       "         [0.7243, 0.3865, 0.4297, 0.0069, 0.3211],\n",
       "         [0.9084, 0.0009, 0.5393, 0.4543, 0.1057]], requires_grad=True),\n",
       " tensor([[0.3158, 0.7038, 0.3901, 0.7456, 0.8604],\n",
       "         [0.4813, 0.8476, 0.9554, 0.9591, 0.9958],\n",
       "         [0.5856, 0.4295, 0.9476, 0.6895, 0.3585]], requires_grad=True))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3189, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.mean(a * b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[0.0211, 0.0469, 0.0260, 0.0497, 0.0574],\n",
      "        [0.0321, 0.0565, 0.0637, 0.0639, 0.0664],\n",
      "        [0.0390, 0.0286, 0.0632, 0.0460, 0.0239]])\n",
      "\n",
      "b.grad=tensor([[2.1862e-02, 3.9616e-02, 4.3297e-02, 5.3840e-02, 4.6404e-02],\n",
      "        [4.8289e-02, 2.5765e-02, 2.8644e-02, 4.6089e-04, 2.1404e-02],\n",
      "        [6.0560e-02, 6.2525e-05, 3.5952e-02, 3.0284e-02, 7.0479e-03]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # b / (3 * 5)\n",
    "print(f'{b.grad=}\\n')  # a / (3 * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1862e-02, 3.9616e-02, 4.3297e-02, 5.3840e-02, 4.6404e-02],\n",
       "        [4.8289e-02, 2.5765e-02, 2.8644e-02, 4.6089e-04, 2.1404e-02],\n",
       "        [6.0560e-02, 6.2525e-05, 3.5952e-02, 3.0284e-02, 7.0479e-03]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0211, 0.0469, 0.0260, 0.0497, 0.0574],\n",
       "        [0.0321, 0.0565, 0.0637, 0.0639, 0.0664],\n",
       "        [0.0390, 0.0286, 0.0632, 0.0460, 0.0239]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[0.9249, 0.5178, 0.4018, 0.5241, 0.2307],\n",
      "        [0.4165, 0.5878, 0.6177, 0.4664, 0.3297],\n",
      "        [0.2164, 0.3102, 0.1307, 0.8902, 0.9583]], requires_grad=True)\n",
      "\n",
      "a.grad=None\n",
      "\n",
      "a.grad=tensor([[1.8499, 1.0356, 0.8036, 1.0483, 0.4613],\n",
      "        [0.8329, 1.1757, 1.2353, 0.9329, 0.6594],\n",
      "        [0.4329, 0.6204, 0.2613, 1.7805, 1.9166]])\n",
      "\n",
      "a.grad=tensor([[2.8499, 2.0356, 1.8036, 2.0483, 1.4613],\n",
      "        [1.8329, 2.1757, 2.2353, 1.9329, 1.6594],\n",
      "        [1.4329, 1.6204, 1.2613, 2.7805, 2.9166]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "print(f'{a=}\\n')\n",
    "\n",
    "loss1 = torch.sum(a ** 2) # 2a\n",
    "loss2 = torch.sum(a) # 1\n",
    "\n",
    "print(f'{a.grad=}\\n')\n",
    "\n",
    "loss1.backward()\n",
    "\n",
    "print(f'{a.grad=}\\n')\n",
    "\n",
    "loss2.backward()\n",
    "\n",
    "print(f'{a.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*a=tensor([[1.8499, 1.0356, 0.8036, 1.0483, 0.4613],\n",
      "        [0.8329, 1.1757, 1.2353, 0.9329, 0.6594],\n",
      "        [0.4329, 0.6204, 0.2613, 1.7805, 1.9166]], grad_fn=<MulBackward0>)\n",
      "\n",
      "2*a+1=tensor([[2.8499, 2.0356, 1.8036, 2.0483, 1.4613],\n",
      "        [1.8329, 2.1757, 2.2353, 1.9329, 1.6594],\n",
      "        [1.4329, 1.6204, 1.2613, 2.7805, 2.9166]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'{2*a=}\\n')\n",
    "print(f'{2*a+1=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9585, 0.8137, 0.7837, 0.3117, 0.5308],\n",
       "         [0.5991, 0.7310, 0.9645, 0.7374, 0.7492],\n",
       "         [0.5522, 0.8426, 0.0137, 0.8135, 0.0080]], requires_grad=True),\n",
       " tensor([[0.0779, 0.3624, 0.0938, 0.1770, 0.2233],\n",
       "         [0.3437, 0.3984, 0.9332, 0.7502, 0.0515],\n",
       "         [0.3722, 0.6910, 0.0804, 0.5425, 0.1549]]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=False)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1571, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # all ones\n",
    "print(f'{b.grad=}\\n')  # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4873, 0.0094, 0.3410, 0.6388, 0.3698],\n",
       "         [0.3318, 0.3989, 0.4540, 0.8396, 0.7872],\n",
       "         [0.2356, 0.5495, 0.3216, 0.0462, 0.1471]], requires_grad=True),\n",
       " tensor([[0.9219, 0.8603, 0.0196, 0.7148, 0.3156],\n",
       "         [0.4160, 0.4039, 0.9161, 0.9858, 0.4589],\n",
       "         [0.9566, 0.1626, 0.5449, 0.9074, 0.7661]], requires_grad=True))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.3924)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4194, 0.7270, 0.6554, 0.8120, 0.2465],\n",
       "         [0.3338, 0.5301, 0.4804, 0.7732, 0.9213],\n",
       "         [0.9066, 0.9495, 0.0488, 0.6136, 0.7182]], requires_grad=True),\n",
       " tensor([[0.2233, 0.5705, 0.0119, 0.8199, 0.7503],\n",
       "         [0.4163, 0.5326, 0.9683, 0.9022, 0.6971],\n",
       "         [0.3597, 0.9322, 0.4627, 0.8851, 0.0571]], requires_grad=True))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5469)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [107]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(15.7352)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [108]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(a \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.sum(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.7352, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 = torch.sum(a + b)\n",
    "\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "b.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(19.4477)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [113]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(a \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.sum(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.4477)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 = torch.sum(a + b)\n",
    "\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def foo():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.mean(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(0.9730)\n"
     ]
    }
   ],
   "source": [
    "a, b = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0091, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def foo():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.mean(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(1.0910)\n"
     ]
    }
   ],
   "source": [
    "a, b = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0245)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Полносвязные слои и функции активации в `PyTorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Полносвязный слой\n",
    "\n",
    ">$y_j = \\sum\\limits_{i=1}^{n}x_iw_{ji} + b_j$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "layer = nn.Linear(in_features=5, out_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=3, bias=True)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1036, -0.0857,  0.1891,  0.1899,  0.0061],\n",
       "        [-0.2206, -0.1663, -0.0230, -0.0566, -0.0549],\n",
       "        [-0.1880, -0.2969, -0.1313,  0.2695,  0.4457]], requires_grad=True)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1110, -0.0980, -0.0736], requires_grad=True)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(in_features=5, out_features=3, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3352,  0.3743,  0.8121], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Функции активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Сигмоида $f(x) = \\dfrac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.7887, -2.3923,  0.1006, -1.6202,  1.0863])\n",
      "tensor([0.1432, 0.0838, 0.5251, 0.1652, 0.7477])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ReLU $f(x) = \\max(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6977,  0.5788, -1.0462, -0.7537,  0.8037])\n",
      "tensor([0.0000, 0.5788, 0.0000, 0.0000, 0.8037])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Leaky ReLU $f(x) = \\max(0, x) + \\alpha \\min(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.LeakyReLU(negative_slope=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4079, -0.8732,  0.4297,  0.5221,  0.3086])\n",
      "tensor([ 1.4079e+00, -8.7321e-04,  4.2974e-01,  5.2209e-01,  3.0863e-01])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Градиентный спуск своими руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_objects = 300\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "w_true = torch.randn(n_features)\n",
    "b_true = torch.randn(1)\n",
    "\n",
    "x = (torch.rand(n_objects, n_features) - 0.5) * 10 * (torch.arange(n_features) * 2 + 1)\n",
    "y = torch.matmul(x, w_true) + torch.randn(n_objects) + b_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_steps = 200\n",
    "step_size = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 23.65750\n",
      "MSE на шаге 2 15.53605\n",
      "MSE на шаге 3 12.11575\n",
      "MSE на шаге 4 10.36995\n",
      "MSE на шаге 5 9.30175\n",
      "MSE на шаге 6 8.55659\n",
      "MSE на шаге 7 7.99169\n",
      "MSE на шаге 8 7.53972\n",
      "MSE на шаге 9 7.16377\n",
      "MSE на шаге 10 6.84131\n",
      "MSE на шаге 11 6.55765\n",
      "MSE на шаге 12 6.30292\n",
      "MSE на шаге 13 6.07033\n",
      "MSE на шаге 14 5.85515\n",
      "MSE на шаге 15 5.65409\n",
      "MSE на шаге 16 5.46479\n",
      "MSE на шаге 17 5.28555\n",
      "MSE на шаге 18 5.11514\n",
      "MSE на шаге 19 4.95265\n",
      "MSE на шаге 20 4.79735\n",
      "MSE на шаге 21 4.64871\n",
      "MSE на шаге 31 3.45555\n",
      "MSE на шаге 41 2.65631\n",
      "MSE на шаге 51 2.11964\n",
      "MSE на шаге 61 1.75925\n",
      "MSE на шаге 71 1.51724\n",
      "MSE на шаге 81 1.35471\n",
      "MSE на шаге 91 1.24558\n",
      "MSE на шаге 101 1.17229\n",
      "MSE на шаге 111 1.12307\n",
      "MSE на шаге 121 1.09002\n",
      "MSE на шаге 131 1.06782\n",
      "MSE на шаге 141 1.05292\n",
      "MSE на шаге 151 1.04291\n",
      "MSE на шаге 161 1.03619\n",
      "MSE на шаге 171 1.03168\n",
      "MSE на шаге 181 1.02864\n",
      "MSE на шаге 191 1.02661\n"
     ]
    }
   ],
   "source": [
    "w = torch.rand(n_features, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = torch.matmul(x, w) + b\n",
    "    \n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * step_size\n",
    "        b -= b.grad * step_size\n",
    "\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 67.76115\n",
      "MSE на шаге 2 42.90440\n",
      "MSE на шаге 3 34.78294\n",
      "MSE на шаге 4 31.90576\n",
      "MSE на шаге 5 30.72448\n",
      "MSE на шаге 6 30.12742\n",
      "MSE на шаге 7 29.75548\n",
      "MSE на шаге 8 29.48539\n",
      "MSE на шаге 9 29.27014\n",
      "MSE на шаге 10 29.08890\n",
      "MSE на шаге 11 28.93083\n",
      "MSE на шаге 12 28.78951\n",
      "MSE на шаге 13 28.66082\n",
      "MSE на шаге 14 28.54199\n",
      "MSE на шаге 15 28.43109\n",
      "MSE на шаге 16 28.32677\n",
      "MSE на шаге 17 28.22807\n",
      "MSE на шаге 18 28.13428\n",
      "MSE на шаге 19 28.04487\n",
      "MSE на шаге 20 27.95945\n",
      "MSE на шаге 21 27.87770\n",
      "MSE на шаге 31 27.22173\n",
      "MSE на шаге 41 26.78238\n",
      "MSE на шаге 51 26.48738\n",
      "MSE на шаге 61 26.28927\n",
      "MSE на шаге 71 26.15623\n",
      "MSE на шаге 81 26.06689\n",
      "MSE на шаге 91 26.00690\n",
      "MSE на шаге 101 25.96661\n",
      "MSE на шаге 111 25.93955\n",
      "MSE на шаге 121 25.92138\n",
      "MSE на шаге 131 25.90918\n",
      "MSE на шаге 141 25.90099\n",
      "MSE на шаге 151 25.89549\n",
      "MSE на шаге 161 25.89179\n",
      "MSE на шаге 171 25.88931\n",
      "MSE на шаге 181 25.88764\n",
      "MSE на шаге 191 25.88653\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x)\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "#     layer.weight.grad.zero_()\n",
    "#     layer.bias.grad.zero_()\n",
    "    \n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7298],\n",
       "        [-2.7292],\n",
       "        [-2.7318],\n",
       "        [-2.7311],\n",
       "        [-2.7278],\n",
       "        [-2.7308],\n",
       "        [-2.7292],\n",
       "        [-2.7330],\n",
       "        [-2.7296],\n",
       "        [-2.7168],\n",
       "        [-2.7313],\n",
       "        [-2.7349],\n",
       "        [-2.7305],\n",
       "        [-2.7286],\n",
       "        [-2.7363],\n",
       "        [-2.7269],\n",
       "        [-2.7251],\n",
       "        [-2.7222],\n",
       "        [-2.7308],\n",
       "        [-2.7260],\n",
       "        [-2.7310],\n",
       "        [-2.7279],\n",
       "        [-2.7256],\n",
       "        [-2.7202],\n",
       "        [-2.7229],\n",
       "        [-2.7306],\n",
       "        [-2.7265],\n",
       "        [-2.7337],\n",
       "        [-2.7233],\n",
       "        [-2.7287],\n",
       "        [-2.7279],\n",
       "        [-2.7318],\n",
       "        [-2.7289],\n",
       "        [-2.7257],\n",
       "        [-2.7223],\n",
       "        [-2.7243],\n",
       "        [-2.7390],\n",
       "        [-2.7176],\n",
       "        [-2.7257],\n",
       "        [-2.7269],\n",
       "        [-2.7223],\n",
       "        [-2.7276],\n",
       "        [-2.7244],\n",
       "        [-2.7321],\n",
       "        [-2.7252],\n",
       "        [-2.7309],\n",
       "        [-2.7242],\n",
       "        [-2.7299],\n",
       "        [-2.7348],\n",
       "        [-2.7298],\n",
       "        [-2.7290],\n",
       "        [-2.7337],\n",
       "        [-2.7367],\n",
       "        [-2.7189],\n",
       "        [-2.7300],\n",
       "        [-2.7280],\n",
       "        [-2.7253],\n",
       "        [-2.7337],\n",
       "        [-2.7322],\n",
       "        [-2.7274],\n",
       "        [-2.7266],\n",
       "        [-2.7305],\n",
       "        [-2.7325],\n",
       "        [-2.7221],\n",
       "        [-2.7273],\n",
       "        [-2.7294],\n",
       "        [-2.7239],\n",
       "        [-2.7206],\n",
       "        [-2.7323],\n",
       "        [-2.7229],\n",
       "        [-2.7279],\n",
       "        [-2.7308],\n",
       "        [-2.7320],\n",
       "        [-2.7193],\n",
       "        [-2.7196],\n",
       "        [-2.7275],\n",
       "        [-2.7284],\n",
       "        [-2.7304],\n",
       "        [-2.7291],\n",
       "        [-2.7330],\n",
       "        [-2.7342],\n",
       "        [-2.7195],\n",
       "        [-2.7230],\n",
       "        [-2.7376],\n",
       "        [-2.7266],\n",
       "        [-2.7182],\n",
       "        [-2.7261],\n",
       "        [-2.7240],\n",
       "        [-2.7260],\n",
       "        [-2.7287],\n",
       "        [-2.7365],\n",
       "        [-2.7253],\n",
       "        [-2.7333],\n",
       "        [-2.7305],\n",
       "        [-2.7338],\n",
       "        [-2.7322],\n",
       "        [-2.7306],\n",
       "        [-2.7302],\n",
       "        [-2.7355],\n",
       "        [-2.7345],\n",
       "        [-2.7258],\n",
       "        [-2.7270],\n",
       "        [-2.7259],\n",
       "        [-2.7286],\n",
       "        [-2.7312],\n",
       "        [-2.7181],\n",
       "        [-2.7331],\n",
       "        [-2.7254],\n",
       "        [-2.7366],\n",
       "        [-2.7318],\n",
       "        [-2.7297],\n",
       "        [-2.7317],\n",
       "        [-2.7312],\n",
       "        [-2.7204],\n",
       "        [-2.7212],\n",
       "        [-2.7324],\n",
       "        [-2.7290],\n",
       "        [-2.7303],\n",
       "        [-2.7195],\n",
       "        [-2.7187],\n",
       "        [-2.7270],\n",
       "        [-2.7324],\n",
       "        [-2.7204],\n",
       "        [-2.7305],\n",
       "        [-2.7373],\n",
       "        [-2.7283],\n",
       "        [-2.7349],\n",
       "        [-2.7276],\n",
       "        [-2.7236],\n",
       "        [-2.7314],\n",
       "        [-2.7231],\n",
       "        [-2.7292],\n",
       "        [-2.7359],\n",
       "        [-2.7278],\n",
       "        [-2.7184],\n",
       "        [-2.7301],\n",
       "        [-2.7245],\n",
       "        [-2.7376],\n",
       "        [-2.7306],\n",
       "        [-2.7318],\n",
       "        [-2.7294],\n",
       "        [-2.7340],\n",
       "        [-2.7255],\n",
       "        [-2.7266],\n",
       "        [-2.7193],\n",
       "        [-2.7233],\n",
       "        [-2.7323],\n",
       "        [-2.7328],\n",
       "        [-2.7229],\n",
       "        [-2.7287],\n",
       "        [-2.7244],\n",
       "        [-2.7332],\n",
       "        [-2.7236],\n",
       "        [-2.7283],\n",
       "        [-2.7318],\n",
       "        [-2.7252],\n",
       "        [-2.7279],\n",
       "        [-2.7341],\n",
       "        [-2.7271],\n",
       "        [-2.7242],\n",
       "        [-2.7222],\n",
       "        [-2.7299],\n",
       "        [-2.7236],\n",
       "        [-2.7349],\n",
       "        [-2.7271],\n",
       "        [-2.7251],\n",
       "        [-2.7279],\n",
       "        [-2.7245],\n",
       "        [-2.7366],\n",
       "        [-2.7269],\n",
       "        [-2.7339],\n",
       "        [-2.7342],\n",
       "        [-2.7311],\n",
       "        [-2.7286],\n",
       "        [-2.7327],\n",
       "        [-2.7341],\n",
       "        [-2.7282],\n",
       "        [-2.7376],\n",
       "        [-2.7284],\n",
       "        [-2.7178],\n",
       "        [-2.7322],\n",
       "        [-2.7360],\n",
       "        [-2.7312],\n",
       "        [-2.7323],\n",
       "        [-2.7257],\n",
       "        [-2.7263],\n",
       "        [-2.7201],\n",
       "        [-2.7289],\n",
       "        [-2.7302],\n",
       "        [-2.7270],\n",
       "        [-2.7328],\n",
       "        [-2.7259],\n",
       "        [-2.7294],\n",
       "        [-2.7368],\n",
       "        [-2.7221],\n",
       "        [-2.7358],\n",
       "        [-2.7235],\n",
       "        [-2.7357],\n",
       "        [-2.7213],\n",
       "        [-2.7244],\n",
       "        [-2.7333],\n",
       "        [-2.7313],\n",
       "        [-2.7218],\n",
       "        [-2.7315],\n",
       "        [-2.7290],\n",
       "        [-2.7316],\n",
       "        [-2.7246],\n",
       "        [-2.7298],\n",
       "        [-2.7199],\n",
       "        [-2.7257],\n",
       "        [-2.7328],\n",
       "        [-2.7349],\n",
       "        [-2.7220],\n",
       "        [-2.7311],\n",
       "        [-2.7227],\n",
       "        [-2.7199],\n",
       "        [-2.7304],\n",
       "        [-2.7326],\n",
       "        [-2.7388],\n",
       "        [-2.7322],\n",
       "        [-2.7233],\n",
       "        [-2.7243],\n",
       "        [-2.7230],\n",
       "        [-2.7367],\n",
       "        [-2.7298],\n",
       "        [-2.7324],\n",
       "        [-2.7333],\n",
       "        [-2.7226],\n",
       "        [-2.7281],\n",
       "        [-2.7237],\n",
       "        [-2.7322],\n",
       "        [-2.7293],\n",
       "        [-2.7231],\n",
       "        [-2.7207],\n",
       "        [-2.7364],\n",
       "        [-2.7344],\n",
       "        [-2.7308],\n",
       "        [-2.7364],\n",
       "        [-2.7323],\n",
       "        [-2.7260],\n",
       "        [-2.7262],\n",
       "        [-2.7301],\n",
       "        [-2.7209],\n",
       "        [-2.7212],\n",
       "        [-2.7315],\n",
       "        [-2.7257],\n",
       "        [-2.7381],\n",
       "        [-2.7270],\n",
       "        [-2.7191],\n",
       "        [-2.7275],\n",
       "        [-2.7317],\n",
       "        [-2.7330],\n",
       "        [-2.7282],\n",
       "        [-2.7277],\n",
       "        [-2.7319],\n",
       "        [-2.7232],\n",
       "        [-2.7245],\n",
       "        [-2.7219],\n",
       "        [-2.7298],\n",
       "        [-2.7199],\n",
       "        [-2.7256],\n",
       "        [-2.7252],\n",
       "        [-2.7296],\n",
       "        [-2.7335],\n",
       "        [-2.7267],\n",
       "        [-2.7314],\n",
       "        [-2.7328],\n",
       "        [-2.7273],\n",
       "        [-2.7308],\n",
       "        [-2.7331],\n",
       "        [-2.7353],\n",
       "        [-2.7304],\n",
       "        [-2.7176],\n",
       "        [-2.7261],\n",
       "        [-2.7284],\n",
       "        [-2.7234],\n",
       "        [-2.7323],\n",
       "        [-2.7328],\n",
       "        [-2.7371],\n",
       "        [-2.7287],\n",
       "        [-2.7245],\n",
       "        [-2.7244],\n",
       "        [-2.7270],\n",
       "        [-2.7324],\n",
       "        [-2.7260],\n",
       "        [-2.7295],\n",
       "        [-2.7261],\n",
       "        [-2.7358],\n",
       "        [-2.7261],\n",
       "        [-2.7282],\n",
       "        [-2.7257],\n",
       "        [-2.7248],\n",
       "        [-2.7390],\n",
       "        [-2.7267],\n",
       "        [-2.7387],\n",
       "        [-2.7252],\n",
       "        [-2.7323],\n",
       "        [-2.7375],\n",
       "        [-2.7289],\n",
       "        [-2.7218]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 1])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 300])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(layer(x) - y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(layer(x).ravel() - y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 82.40031\n",
      "MSE на шаге 2 34.83059\n",
      "MSE на шаге 3 18.67639\n",
      "MSE на шаге 4 12.57786\n",
      "MSE на шаге 5 9.87140\n",
      "MSE на шаге 6 8.42349\n",
      "MSE на шаге 7 7.51441\n",
      "MSE на шаге 8 6.87739\n",
      "MSE на шаге 9 6.39865\n",
      "MSE на шаге 10 6.02125\n",
      "MSE на шаге 11 5.71254\n",
      "MSE на шаге 12 5.45201\n",
      "MSE на шаге 13 5.22616\n",
      "MSE на шаге 14 5.02584\n",
      "MSE на шаге 15 4.84476\n",
      "MSE на шаге 16 4.67857\n",
      "MSE на шаге 17 4.52421\n",
      "MSE на шаге 18 4.37955\n",
      "MSE на шаге 19 4.24304\n",
      "MSE на шаге 20 4.11358\n",
      "MSE на шаге 21 3.99035\n",
      "MSE на шаге 31 3.01138\n",
      "MSE на шаге 41 2.35799\n",
      "MSE на шаге 51 1.91931\n",
      "MSE на шаге 61 1.62472\n",
      "MSE на шаге 71 1.42689\n",
      "MSE на шаге 81 1.29405\n",
      "MSE на шаге 91 1.20484\n",
      "MSE на шаге 101 1.14493\n",
      "MSE на шаге 111 1.10470\n",
      "MSE на шаге 121 1.07768\n",
      "MSE на шаге 131 1.05954\n",
      "MSE на шаге 141 1.04736\n",
      "MSE на шаге 151 1.03917\n",
      "MSE на шаге 161 1.03368\n",
      "MSE на шаге 171 1.02999\n",
      "MSE на шаге 181 1.02751\n",
      "MSE на шаге 191 1.02585\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 5\n",
    "n_objects = 300\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "w_true = torch.randn(n_features)\n",
    "\n",
    "x = (torch.rand(n_objects, n_features) - 0.5) * 10 * (torch.arange(n_features) * 2 + 1)\n",
    "y = torch.matmul(x, w_true) + torch.randn(n_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2000\n",
    "step_size = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 2052.12988\n",
      "MSE на шаге 2 606.21967\n",
      "MSE на шаге 3 207.55234\n",
      "MSE на шаге 4 82.47279\n",
      "MSE на шаге 5 40.03379\n",
      "MSE на шаге 6 24.82249\n",
      "MSE на шаге 7 19.02928\n",
      "MSE на шаге 8 16.58483\n",
      "MSE на шаге 9 15.35648\n",
      "MSE на шаге 10 14.58259\n",
      "MSE на шаге 11 13.98817\n",
      "MSE на шаге 12 13.47272\n",
      "MSE на шаге 13 12.99872\n",
      "MSE на шаге 14 12.55160\n",
      "MSE на шаге 15 12.12528\n",
      "MSE на шаге 16 11.71682\n",
      "MSE на шаге 17 11.32460\n",
      "MSE на шаге 18 10.94749\n",
      "MSE на шаге 19 10.58467\n",
      "MSE на шаге 20 10.23542\n",
      "MSE на шаге 51 3.92278\n",
      "MSE на шаге 101 1.43192\n",
      "MSE на шаге 151 1.02777\n",
      "MSE на шаге 201 0.95445\n",
      "MSE на шаге 251 0.93498\n",
      "MSE на шаге 301 0.92541\n",
      "MSE на шаге 351 0.91856\n",
      "MSE на шаге 401 0.91310\n",
      "MSE на шаге 451 0.90864\n",
      "MSE на шаге 501 0.90498\n",
      "MSE на шаге 551 0.90198\n",
      "MSE на шаге 601 0.89951\n",
      "MSE на шаге 651 0.89748\n",
      "MSE на шаге 701 0.89582\n",
      "MSE на шаге 751 0.89445\n",
      "MSE на шаге 801 0.89333\n",
      "MSE на шаге 851 0.89241\n",
      "MSE на шаге 901 0.89165\n",
      "MSE на шаге 951 0.89103\n",
      "MSE на шаге 1001 0.89052\n",
      "MSE на шаге 1051 0.89010\n",
      "MSE на шаге 1101 0.88976\n",
      "MSE на шаге 1151 0.88948\n",
      "MSE на шаге 1201 0.88925\n",
      "MSE на шаге 1251 0.88906\n",
      "MSE на шаге 1301 0.88890\n",
      "MSE на шаге 1351 0.88877\n",
      "MSE на шаге 1401 0.88867\n",
      "MSE на шаге 1451 0.88858\n",
      "MSE на шаге 1501 0.88851\n",
      "MSE на шаге 1551 0.88845\n",
      "MSE на шаге 1601 0.88840\n",
      "MSE на шаге 1651 0.88836\n",
      "MSE на шаге 1701 0.88833\n",
      "MSE на шаге 1751 0.88830\n",
      "MSE на шаге 1801 0.88828\n",
      "MSE на шаге 1851 0.88826\n",
      "MSE на шаге 1901 0.88825\n",
      "MSE на шаге 1951 0.88824\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 50 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2000\n",
    "step_size = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 1942.53650\n",
      "MSE на шаге 2 1676.33215\n",
      "MSE на шаге 3 1462.73621\n",
      "MSE на шаге 4 1252.29272\n",
      "MSE на шаге 5 1084.68542\n",
      "MSE на шаге 6 969.74554\n",
      "MSE на шаге 7 892.42969\n",
      "MSE на шаге 8 837.99945\n",
      "MSE на шаге 9 791.54950\n",
      "MSE на шаге 10 736.85345\n",
      "MSE на шаге 11 660.66931\n",
      "MSE на шаге 12 552.94324\n",
      "MSE на шаге 13 420.34772\n",
      "MSE на шаге 14 295.80695\n",
      "MSE на шаге 15 213.59764\n",
      "MSE на шаге 16 170.35097\n",
      "MSE на шаге 17 140.10820\n",
      "MSE на шаге 18 116.49634\n",
      "MSE на шаге 19 95.18255\n",
      "MSE на шаге 20 74.59748\n",
      "MSE на шаге 51 9.93147\n",
      "MSE на шаге 101 3.76550\n",
      "MSE на шаге 151 1.81388\n",
      "MSE на шаге 201 1.21714\n",
      "MSE на шаге 251 1.03567\n",
      "MSE на шаге 301 0.97787\n",
      "MSE на шаге 351 0.95785\n",
      "MSE на шаге 401 0.94890\n",
      "MSE на шаге 451 0.94261\n",
      "MSE на шаге 501 0.93729\n",
      "MSE на шаге 551 0.93249\n",
      "MSE на шаге 601 0.92810\n",
      "MSE на шаге 651 0.92405\n",
      "MSE на шаге 701 0.92031\n",
      "MSE на шаге 751 0.91686\n",
      "MSE на шаге 801 0.91367\n",
      "MSE на шаге 851 0.91071\n",
      "MSE на шаге 901 0.90798\n",
      "MSE на шаге 951 0.90545\n",
      "MSE на шаге 1001 0.90310\n",
      "MSE на шаге 1051 0.90093\n",
      "MSE на шаге 1101 0.89892\n",
      "MSE на шаге 1151 0.89705\n",
      "MSE на шаге 1201 0.89531\n",
      "MSE на шаге 1251 0.89371\n",
      "MSE на шаге 1301 0.89224\n",
      "MSE на шаге 1351 0.89088\n",
      "MSE на шаге 1401 0.88962\n",
      "MSE на шаге 1451 0.88845\n",
      "MSE на шаге 1501 0.88737\n",
      "MSE на шаге 1551 0.88636\n",
      "MSE на шаге 1601 0.88543\n",
      "MSE на шаге 1651 0.88456\n",
      "MSE на шаге 1701 0.88375\n",
      "MSE на шаге 1751 0.88300\n",
      "MSE на шаге 1801 0.88231\n",
      "MSE на шаге 1851 0.88166\n",
      "MSE на шаге 1901 0.88105\n",
      "MSE на шаге 1951 0.88049\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=n_features, out_features=3)\n",
    "layer2 = nn.Linear(in_features=3, out_features=1)\n",
    "activation = nn.ReLU()\n",
    "\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer2(activation(layer1(x))).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "\n",
    "    if i < 20 or i % 50 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer1.weight -= layer1.weight.grad * step_size\n",
    "        layer1.bias -= layer1.bias.grad * step_size\n",
    "        layer2.weight -= layer2.weight.grad * step_size\n",
    "        layer2.bias -= layer2.bias.grad * step_size\n",
    "\n",
    "    layer1.zero_grad()\n",
    "    layer2.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Seminar 1. Intro to DL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
